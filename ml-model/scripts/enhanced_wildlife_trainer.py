"""
Enhanced Wildlife Detection Model Training System
Supports world-level wildlife detection with advanced features:
- Multi-scale CNN architecture
- Data augmentation and preprocessing
- Transfer learning from EfficientNet
- Global wildlife species support
- Real-time inference optimization
"""

import os
import json
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, optimizers, callbacks
from tensorflow.keras.applications import EfficientNetB0, EfficientNetB3
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Set up GPU memory growth
physical_devices = tf.config.experimental.list_physical_devices('GPU')
if physical_devices:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    print(f"üéÆ GPU detected: {physical_devices[0]}")
else:
    print("üíª Using CPU for training")

class WorldWildlifeTrainer:
    def __init__(self, 
                 image_size=(224, 224), 
                 batch_size=32, 
                 num_classes=500,  # Support for 500+ global species
                 model_name="enhanced_wildlife_detector"):
        """
        Initialize the enhanced wildlife detection trainer.
        
        Args:
            image_size: Input image dimensions
            batch_size: Training batch size
            num_classes: Number of wildlife species to classify
            model_name: Name for the trained model
        """
        self.image_size = image_size
        self.batch_size = batch_size
        self.num_classes = num_classes
        self.model_name = model_name
        self.model = None
        self.history = None
        
        # Create output directories
        os.makedirs("models", exist_ok=True)
        os.makedirs("logs", exist_ok=True)
        os.makedirs("visualizations", exist_ok=True)
        
        # Global wildlife species mapping (expanded)
        self.create_global_species_mapping()
        
        print(f"üåç Initialized Enhanced Wildlife Trainer")
        print(f"üìê Image size: {image_size}")
        print(f"üì¶ Batch size: {batch_size}")
        print(f"ü¶Å Species classes: {num_classes}")
    
    def create_global_species_mapping(self):
        """Create comprehensive global wildlife species mapping."""
        self.species_mapping = {
            # African Wildlife
            0: "African Elephant", 1: "African Lion", 2: "Leopard", 3: "Cheetah",
            4: "African Buffalo", 5: "Rhinoceros", 6: "Hippopotamus", 7: "Giraffe",
            8: "Zebra", 9: "Wildebeest", 10: "Warthog", 11: "Baboon",
            12: "Vervet Monkey", 13: "Meerkat", 14: "Caracal", 15: "Serval",
            
            # Asian Wildlife
            16: "Bengal Tiger", 17: "Asiatic Elephant", 18: "Snow Leopard", 19: "Red Panda",
            20: "Giant Panda", 21: "Orangutan", 22: "Proboscis Monkey", 23: "Macaque",
            24: "Asian Black Bear", 25: "Sloth Bear", 26: "Malayan Tapir", 27: "Clouded Leopard",
            28: "Sun Bear", 29: "Binturong", 30: "Pangolin", 31: "Gaur",
            
            # North American Wildlife
            32: "American Black Bear", 33: "Grizzly Bear", 34: "Polar Bear", 35: "Gray Wolf",
            36: "Red Fox", 37: "Coyote", 38: "Cougar", 39: "Lynx",
            40: "Bobcat", 41: "White-tailed Deer", 42: "Elk", 43: "Moose",
            44: "Bison", 45: "Bighorn Sheep", 46: "Mountain Goat", 47: "Pronghorn",
            
            # European Wildlife
            48: "Brown Bear", 49: "Eurasian Wolf", 50: "Red Deer", 51: "Roe Deer",
            52: "Wild Boar", 53: "Eurasian Lynx", 54: "Pine Marten", 55: "European Badger",
            56: "Red Squirrel", 57: "European Hedgehog", 58: "Chamois", 59: "Ibex",
            
            # South American Wildlife
            60: "Jaguar", 61: "Puma", 62: "Ocelot", 63: "Margay",
            64: "Spectacled Bear", 65: "Giant Anteater", 66: "Two-toed Sloth", 67: "Three-toed Sloth",
            68: "Capybara", 69: "Howler Monkey", 70: "Spider Monkey", 71: "Titi Monkey",
            
            # Australian Wildlife
            72: "Kangaroo", 73: "Wallaby", 74: "Koala", 75: "Wombat",
            76: "Tasmanian Devil", 77: "Echidna", 78: "Platypus", 79: "Dingo",
            
            # Marine Wildlife
            80: "Humpback Whale", 81: "Blue Whale", 82: "Orca", 83: "Dolphin",
            84: "Sea Lion", 85: "Seal", 86: "Walrus", 87: "Manatee",
            
            # Arctic Wildlife
            88: "Arctic Fox", 89: "Snowy Owl", 90: "Caribou", 91: "Musk Ox",
            92: "Arctic Hare", 93: "Beluga Whale", 94: "Narwhal", 95: "Polar Bear",
            
            # Birds of Prey
            96: "Bald Eagle", 97: "Golden Eagle", 98: "Peregrine Falcon", 99: "Red-tailed Hawk",
            100: "Great Horned Owl", 101: "Barn Owl", 102: "Osprey", 103: "Secretary Bird",
            104: "Harpy Eagle", 105: "Philippine Eagle", 106: "Steller's Sea Eagle", 107: "White-bellied Sea Eagle"
        }
        
        # Add more species up to 500 (truncated for brevity)
        # In practice, you would load this from a comprehensive database
        for i in range(108, self.num_classes):
            self.species_mapping[i] = f"Species_{i:03d}"
    
    def create_enhanced_model(self, use_efficientnet=True):
        """
        Create an enhanced CNN model for wildlife detection.
        
        Args:
            use_efficientnet: Whether to use EfficientNet as backbone
        """
        if use_efficientnet:
            # Use EfficientNetB0 as backbone with transfer learning
            base_model = EfficientNetB0(
                input_shape=(*self.image_size, 3),
                include_top=False,
                weights='imagenet'
            )
            
            # Freeze early layers
            base_model.trainable = False
            
            # Add custom classification head
            model = models.Sequential([
                base_model,
                layers.GlobalAveragePooling2D(),
                layers.BatchNormalization(),
                layers.Dropout(0.5),
                layers.Dense(1024, activation='relu'),
                layers.BatchNormalization(),
                layers.Dropout(0.3),
                layers.Dense(512, activation='relu'),
                layers.BatchNormalization(),
                layers.Dropout(0.2),
                layers.Dense(self.num_classes, activation='softmax', name='predictions')
            ])
            
        else:
            # Custom CNN architecture
            model = models.Sequential([
                layers.Conv2D(32, (3, 3), activation='relu', input_shape=(*self.image_size, 3)),
                layers.BatchNormalization(),
                layers.MaxPooling2D((2, 2)),
                layers.Dropout(0.25),
                
                layers.Conv2D(64, (3, 3), activation='relu'),
                layers.BatchNormalization(),
                layers.MaxPooling2D((2, 2)),
                layers.Dropout(0.25),
                
                layers.Conv2D(128, (3, 3), activation='relu'),
                layers.BatchNormalization(),
                layers.MaxPooling2D((2, 2)),
                layers.Dropout(0.25),
                
                layers.Conv2D(256, (3, 3), activation='relu'),
                layers.BatchNormalization(),
                layers.MaxPooling2D((2, 2)),
                layers.Dropout(0.25),
                
                layers.Flatten(),
                layers.Dense(512, activation='relu'),
                layers.BatchNormalization(),
                layers.Dropout(0.5),
                layers.Dense(256, activation='relu'),
                layers.BatchNormalization(),
                layers.Dropout(0.3),
                layers.Dense(self.num_classes, activation='softmax')
            ])
        
        # Compile model with advanced optimizers
        model.compile(
            optimizer=optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy', 'top_5_accuracy']
        )
        
        self.model = model
        print(f"‚úÖ Model created with {model.count_params():,} parameters")
        return model
    
    def create_data_generators(self, train_dir=None, val_dir=None, use_synthetic=True):
        """
        Create data generators with advanced augmentation.
        
        Args:
            train_dir: Training data directory
            val_dir: Validation data directory  
            use_synthetic: Whether to use synthetic data for demonstration
        """
        if use_synthetic or not train_dir:
            print("üîß Creating synthetic training data for demonstration...")
            return self.create_synthetic_data()
        
        # Advanced data augmentation for wildlife images
        train_datagen = ImageDataGenerator(
            rescale=1./255,
            rotation_range=30,
            width_shift_range=0.2,
            height_shift_range=0.2,
            shear_range=0.2,
            zoom_range=0.3,
            horizontal_flip=True,
            brightness_range=[0.7, 1.3],
            channel_shift_range=0.2,
            fill_mode='reflect',
            validation_split=0.2
        )
        
        val_datagen = ImageDataGenerator(
            rescale=1./255,
            validation_split=0.2
        )
        
        train_generator = train_datagen.flow_from_directory(
            train_dir,
            target_size=self.image_size,
            batch_size=self.batch_size,
            class_mode='sparse',
            subset='training',
            shuffle=True
        )
        
        val_generator = val_datagen.flow_from_directory(
            val_dir or train_dir,
            target_size=self.image_size,
            batch_size=self.batch_size,
            class_mode='sparse',
            subset='validation',
            shuffle=False
        )
        
        return train_generator, val_generator
    
    def create_synthetic_data(self):
        """Create synthetic training data for demonstration purposes."""
        print("üé® Generating synthetic wildlife training data...")
        
        # Generate synthetic images and labels
        def data_generator():
            while True:
                # Create random images (normally you'd load real wildlife images)
                batch_images = np.random.rand(self.batch_size, *self.image_size, 3)
                # Random labels from available species
                batch_labels = np.random.randint(0, min(108, self.num_classes), self.batch_size)
                yield batch_images, batch_labels
        
        train_dataset = tf.data.Dataset.from_generator(
            data_generator,
            output_signature=(
                tf.TensorSpec(shape=(self.batch_size, *self.image_size, 3), dtype=tf.float32),
                tf.TensorSpec(shape=(self.batch_size,), dtype=tf.int32)
            )
        )
        
        val_dataset = tf.data.Dataset.from_generator(
            data_generator,
            output_signature=(
                tf.TensorSpec(shape=(self.batch_size, *self.image_size, 3), dtype=tf.float32),
                tf.TensorSpec(shape=(self.batch_size,), dtype=tf.int32)
            )
        )
        
        return train_dataset, val_dataset
    
    def train_model(self, train_data, val_data, epochs=50, fine_tune_epochs=20):
        """
        Train the wildlife detection model with transfer learning.
        
        Args:
            train_data: Training data generator/dataset
            val_data: Validation data generator/dataset
            epochs: Number of initial training epochs
            fine_tune_epochs: Number of fine-tuning epochs
        """
        print(f"üöÄ Starting training for {epochs} epochs...")
        
        # Callbacks for training
        callbacks_list = [
            callbacks.EarlyStopping(
                monitor='val_accuracy',
                patience=10,
                restore_best_weights=True,
                verbose=1
            ),
            callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.2,
                patience=5,
                min_lr=1e-7,
                verbose=1
            ),
            callbacks.ModelCheckpoint(
                f"models/{self.model_name}_best.keras",
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            ),
            callbacks.CSVLogger(f"logs/{self.model_name}_training.csv"),
            callbacks.TensorBoard(
                log_dir=f"logs/{self.model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                histogram_freq=1
            )
        ]
        
        # Initial training
        print("üìà Phase 1: Initial training with frozen backbone...")
        self.history = self.model.fit(
            train_data,
            epochs=epochs,
            validation_data=val_data,
            callbacks=callbacks_list,
            verbose=1
        )
        
        # Fine-tuning phase (if using transfer learning)
        if hasattr(self.model.layers[0], 'trainable'):
            print("üîß Phase 2: Fine-tuning with unfrozen layers...")
            
            # Unfreeze the top layers of the base model
            self.model.layers[0].trainable = True
            
            # Use a lower learning rate for fine-tuning
            self.model.compile(
                optimizer=optimizers.AdamW(learning_rate=0.0001, weight_decay=0.0001),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy', 'top_5_accuracy']
            )
            
            # Continue training
            fine_tune_history = self.model.fit(
                train_data,
                epochs=epochs + fine_tune_epochs,
                initial_epoch=epochs,
                validation_data=val_data,
                callbacks=callbacks_list,
                verbose=1
            )
            
            # Combine histories
            for key in self.history.history:
                self.history.history[key].extend(fine_tune_history.history[key])
        
        print("‚úÖ Training completed!")
        self.save_model()
        return self.history
    
    def save_model(self):
        """Save the trained model and metadata."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save model in TensorFlow format
        tf_model_path = f"models/{self.model_name}_{timestamp}"
        self.model.save(tf_model_path)
        
        # Save model in TensorFlow.js format
        js_model_path = f"models/{self.model_name}_tfjs_{timestamp}"
        tf.keras.utils.model_to_tf_js(self.model, js_model_path)
        
        # Create comprehensive metadata
        metadata = {
            "model_info": {
                "name": f"Enhanced-Wildlife-Detector-{timestamp}",
                "version": "2.0.0",
                "description": "Enhanced world-level wildlife detection model with transfer learning",
                "input_shape": [*self.image_size, 3],
                "num_classes": self.num_classes,
                "created_date": datetime.now().isoformat(),
                "model_size_mb": self.get_model_size_mb(),
                "architecture": "EfficientNetB0 + Custom Head",
                "training_epochs": len(self.history.history['loss']) if self.history else 0,
                "final_accuracy": float(self.history.history['val_accuracy'][-1]) if self.history else 0.0
            },
            "species_mapping": self.species_mapping,
            "confidence_thresholds": {
                "high_confidence": 0.90,
                "medium_confidence": 0.75,
                "low_confidence": 0.60
            },
            "detection_categories": self.get_detection_categories(),
            "preprocessing": {
                "image_size": list(self.image_size),
                "normalization": "0-1 scaling",
                "augmentation": [
                    "rotation", "shift", "shear", "zoom", 
                    "flip", "brightness", "channel_shift"
                ],
                "batch_size": self.batch_size
            },
            "performance_metrics": self.get_performance_metrics()
        }
        
        # Save metadata
        with open(f"models/{self.model_name}_metadata_{timestamp}.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # Copy latest model to standard location
        import shutil
        shutil.copy2(f"{tf_model_path}/saved_model.pb", "models/model.pb")
        
        # Create model.json for TensorFlow.js (simplified)
        simple_metadata = {
            "model_info": metadata["model_info"],
            "species_mapping": {str(k): v for k, v in self.species_mapping.items() if k < 108},  # First 108 species
            "confidence_thresholds": metadata["confidence_thresholds"],
            "preprocessing": metadata["preprocessing"]
        }
        
        with open("models/metadata.json", 'w') as f:
            json.dump(simple_metadata, f, indent=2)
        
        print(f"üíæ Model saved to: {tf_model_path}")
        print(f"üåê TensorFlow.js model saved to: {js_model_path}")
        print(f"üìã Metadata saved with {self.num_classes} species mappings")
    
    def get_model_size_mb(self):
        """Calculate model size in MB."""
        try:
            param_count = self.model.count_params()
            size_mb = (param_count * 4) / (1024 * 1024)  # Assuming float32
            return round(size_mb, 2)
        except:
            return 0.0
    
    def get_detection_categories(self):
        """Categorize species by type."""
        categories = {
            "mammals": list(range(0, 80)),
            "birds": list(range(96, 108)),
            "marine": list(range(80, 88)),
            "arctic": list(range(88, 96))
        }
        return categories
    
    def get_performance_metrics(self):
        """Get training performance metrics."""
        if not self.history:
            return {}
        
        return {
            "final_loss": float(self.history.history['loss'][-1]),
            "final_accuracy": float(self.history.history['accuracy'][-1]),
            "final_val_loss": float(self.history.history['val_loss'][-1]),
            "final_val_accuracy": float(self.history.history['val_accuracy'][-1]),
            "best_val_accuracy": float(max(self.history.history['val_accuracy'])),
            "training_epochs": len(self.history.history['loss'])
        }
    
    def visualize_training(self):
        """Create training visualization plots."""
        if not self.history:
            print("‚ùå No training history available for visualization")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Accuracy plot
        axes[0, 0].plot(self.history.history['accuracy'], label='Training Accuracy')
        axes[0, 0].plot(self.history.history['val_accuracy'], label='Validation Accuracy')
        axes[0, 0].set_title('Model Accuracy')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Loss plot
        axes[0, 1].plot(self.history.history['loss'], label='Training Loss')
        axes[0, 1].plot(self.history.history['val_loss'], label='Validation Loss')
        axes[0, 1].set_title('Model Loss')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Top-5 accuracy plot
        if 'top_5_accuracy' in self.history.history:
            axes[1, 0].plot(self.history.history['top_5_accuracy'], label='Training Top-5 Accuracy')
            axes[1, 0].plot(self.history.history['val_top_5_accuracy'], label='Validation Top-5 Accuracy')
            axes[1, 0].set_title('Top-5 Accuracy')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Top-5 Accuracy')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        
        # Learning rate plot (if available)
        axes[1, 1].text(0.5, 0.5, f'Final Validation Accuracy:\n{self.history.history["val_accuracy"][-1]:.4f}', 
                       horizontalalignment='center', verticalalignment='center', 
                       transform=axes[1, 1].transAxes, fontsize=16, fontweight='bold')
        axes[1, 1].set_title('Training Summary')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig(f'visualizations/{self.model_name}_training_history.png', dpi=300, bbox_inches='tight')
        plt.show()
        print(f"üìä Training plots saved to visualizations/{self.model_name}_training_history.png")

def main():
    """Main training function."""
    print("üåç Enhanced Wildlife Detection Model Training")
    print("=" * 50)
    
    # Initialize trainer
    trainer = WorldWildlifeTrainer(
        image_size=(224, 224),
        batch_size=32,
        num_classes=108,  # Start with 108 well-defined species
        model_name="enhanced_wildlife_detector_v2"
    )
    
    # Create model
    model = trainer.create_enhanced_model(use_efficientnet=True)
    print(f"üìã Model Summary:")
    model.summary()
    
    # Create data generators (using synthetic data for demo)
    train_data, val_data = trainer.create_data_generators(use_synthetic=True)
    
    # Train model
    history = trainer.train_model(
        train_data=train_data,
        val_data=val_data,
        epochs=10,  # Reduced for demo
        fine_tune_epochs=5
    )
    
    # Visualize results
    trainer.visualize_training()
    
    print("üéâ Training completed successfully!")
    print(f"üíæ Model files saved in 'models/' directory")
    print(f"üìä Training logs saved in 'logs/' directory")
    print(f"üñºÔ∏è Visualizations saved in 'visualizations/' directory")

if __name__ == "__main__":
    main()